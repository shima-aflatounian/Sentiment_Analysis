{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shima-aflatounian/Sentiment_Analysis/blob/main/svm_1_to_1_classifier_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol9H6qW4OFFk"
      },
      "source": [
        "including neutral tweets\n",
        "1 to 1 classification: nr of required classifiers : n* (n-1)/2\n",
        "we have 3 class +,-, neutral > n=3 > required nr of classifiers: 3\n",
        "* voting approach for predict the class of test tweet: the class with most majority vote\n",
        "\n",
        "voting section inspired by:\n",
        "https://www.researchgate.net/publication/318484311_A_Study_using_Support_Vector_Machines_to_Classify_the_Sentiments_of_Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7J2JoTL7DtF",
        "outputId": "c6132e0a-c755-42d0-adfd-b19d05284e23"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import pickle\n",
        "import os.path\n",
        "from time import time\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.svm import LinearSVC\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "def get_words_in_tweets(tweets):\n",
        "    all_words = []\n",
        "    for (words, sentiment) in tweets:\n",
        "        all_words.extend(words)\n",
        "    return all_words\n",
        "def get_word_features(wordlist):\n",
        "    wordlist = nltk.FreqDist(wordlist)\n",
        "    word_features = wordlist.keys()\n",
        "    return word_features\n",
        "\n",
        "def extract_features1(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features1:\n",
        "        features['contains(%s)' % word] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "def extract_features2(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features2:\n",
        "        features['contains(%s)' % word] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "def extract_features3(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features3:\n",
        "        features['contains(%s)' % word] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    new_tweet = tweet.lower()\n",
        "    new_tweet = re.sub(r'@\\w+', '', new_tweet) # Remove @s\n",
        "    new_tweet = re.sub(r'#', '', new_tweet) # Remove hashtags\n",
        "    new_tweet = re.sub(r':', ' ', emoji.demojize(new_tweet)) # Turn emojis into words\n",
        "    new_tweet = re.sub(r'http\\S+', 'url',new_tweet) # Remove URLs\n",
        "    new_tweet = re.sub(r'\\$\\S+', 'dollar', new_tweet) # Change dollar amounts to dollar\n",
        "    new_tweet = re.sub(r'[^a-z0-9\\s]', '', new_tweet) # Remove punctuation\n",
        "    new_tweet = re.sub(r'[0-9]+', 'number', new_tweet) # Change number values to number\n",
        "    new_tweet = new_tweet.split(\" \")\n",
        "    new_tweet = list(map(lambda x: ps.stem(x), new_tweet)) # Stemming the words\n",
        "    new_tweet = list(map(lambda x: x.strip(), new_tweet)) # Stripping whitespace from the words\n",
        "    if '' in new_tweet:\n",
        "        new_tweet.remove('')\n",
        "    return new_tweet\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/gabriel-david-orozco/Frequency_words_Spark_vs_Sequential_Python/main/Tweets.csv\")\n",
        "\n",
        "\n",
        "confidence_threshold = 0.7\n",
        "train_percentage = 0.9\n",
        "test_percentage = 1-train_percentage\n",
        "ngbias = 2500   #we introduced a number of negative tweets to remove from the database (as there is a bias to giving negative opinions in Twitter)\n",
        "\n",
        "data = data.drop(data.query(\"airline_sentiment_confidence < @confidence_threshold\").index, axis=0).reset_index(drop=True)\n",
        "tweets_df = pd.concat([data['text'], data['airline_sentiment']], axis=1)\n",
        "tweets_df.isna().sum().sum()\n",
        "tweets_df['airline_sentiment'].value_counts()\n",
        "sentiment_ordering = ['negative', 'neutral', 'positive']\n",
        "\n",
        "tweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].apply(lambda x: sentiment_ordering.index(x))\n",
        "\n",
        "ps = PorterStemmer()\n",
        "tweets1 = []  #this list will contain the training set of the positive vs negative classifier\n",
        "tweets2 = []  #this list will contain the training set of the neutral vs negative classifier\n",
        "tweets3 = []  #this list will contain the training set of the positive vs neutral classifier\n",
        "\n",
        "ttweets1 = []  #this list will contain the testing set of the positive vs negative classifier\n",
        "ttweets2 = []  #this list will contain the testing set of the neutral vs negative classifier\n",
        "ttweets3 = []  #this list will contain the testing set of the positive vs neutral classifier\n",
        "\n",
        "#file = open(\"lexicon.tff\")\n",
        "#lexicon = file.readlines()\n",
        "#unique_words = []\n",
        "#for line in lexicon:\n",
        "#    if(line.split(\" \")[4][9:]==\"y\"):\n",
        "#        if(line.split(\" \")[2][6:] not in unique_words):\n",
        "#            unique_words.append(line.split(\" \")[2][6:])\n",
        "#            if(line.split(\" \")[5][14:-1] == \"neutral\"):\n",
        "#                tweets2.append(((line.split(\" \")[2][6:]), line.split(\" \")[5][14:-1]))\n",
        "#                tweets3.append(((line.split(\" \")[2][6:]), line.split(\" \")[5][14:-1]))\n",
        "#            if(line.split(\" \")[5][14:-1] == \"positive\"):\n",
        "#                tweets1.append(((line.split(\" \")[2][6:]), line.split(\" \")[5][14:-1]))\n",
        "#                tweets3.append(((line.split(\" \")[2][6:]), line.split(\" \")[5][14:-1]))\n",
        "#            if(line.split(\" \")[5][14:-1] == \"negative\"):\n",
        "#                tweets1.append(((line.split(\" \")[2][6:]), line.split(\" \")[5][14:-1]))\n",
        "#                tweets2.append(((line.split(\" \")[2][6:]), line.split(\" \")[5][14:-1]))\n",
        "\n",
        "new_data = pd.read_csv(\"https://raw.githubusercontent.com/gabriel-david-orozco/Frequency_words_Spark_vs_Sequential_Python/main/new_dataset.tsv\", sep='\\t')\n",
        "\n",
        "full_tweets = tweets_df['text']\n",
        "#full_tweets = pd.concat([tweets_df['text'],new_data['TWEETS']]) # the process has not done on text i guess? full_tweets=process_tweet(tweets_df['text])\n",
        "sentiment = tweets_df['airline_sentiment']\n",
        "#sentiment = pd.concat([tweets_df['airline_sentiment'], new_data['SENTIMENTS']])\n",
        "test_tweets = new_data['TWEETS']\n",
        "test_sentiment = new_data['SENTIMENTS']\n",
        "ngcount = 0\n",
        "for s in range(1, len(full_tweets)):\n",
        "    if sentiment[s] == 2:\n",
        "        tweets1.append((process_tweet(full_tweets[s]), 'positive'))\n",
        "    elif sentiment[s] == 0:\n",
        "        ngcount=ngcount+1\n",
        "        if ngcount > ngbias:\n",
        "            tweets1.append((process_tweet(full_tweets[s]), 'negative'))\n",
        "ngcount = 0\n",
        "for s in range(1, len(full_tweets)):\n",
        "    if sentiment[s] == 1:\n",
        "        tweets2.append((process_tweet(full_tweets[s]), 'neutral'))\n",
        "    elif sentiment[s] == 0:\n",
        "        ngcount=ngcount+1\n",
        "        if ngcount > ngbias:\n",
        "            tweets2.append((process_tweet(full_tweets[s]), 'negative'))\n",
        "\n",
        "for s in range(1, len(full_tweets)):\n",
        "    if sentiment[s] == 2:\n",
        "        tweets3.append((process_tweet(full_tweets[s]), 'positive'))\n",
        "    elif sentiment[s] == 1:\n",
        "        tweets3.append((process_tweet(full_tweets[s]), 'neutral'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for s in range(1, len(test_tweets)):\n",
        "    if test_sentiment[s] == 'negative':\n",
        "        ttweets1.append((process_tweet(test_tweets[s]), 'negative'))\n",
        "        ttweets2.append((process_tweet(test_tweets[s]), 'negative'))\n",
        "    elif test_sentiment[s] == 'neutral':\n",
        "        ttweets2.append((process_tweet(test_tweets[s]), 'neutral'))\n",
        "        ttweets3.append((process_tweet(test_tweets[s]), 'neutral'))\n",
        "    elif test_sentiment[s] == 'positive':\n",
        "        ttweets1.append((process_tweet(test_tweets[s]), 'positive'))\n",
        "        ttweets3.append((process_tweet(test_tweets[s]), 'positive'))\n",
        "\n",
        "\n",
        "print(\"full tweets: \", len(full_tweets))\n",
        "\n",
        "print(\"Positive vs negative tweets (training): \", len(tweets1))\n",
        "print(\"Positive vs negative tweets (testing): \", len(ttweets1))\n",
        "\n",
        "print(\"Neutral vs negative tweets (training): \", len(tweets2))\n",
        "print(\"Neutral vs negative tweets (testing): \", len(ttweets2))\n",
        "\n",
        "print(\"Positive vs neutral tweets (training): \", len(tweets3))\n",
        "print(\"Positive vs neutral tweets (testing): \", len(ttweets2))\n",
        "\n",
        "#Ltrain_samples1 = int(len(tweets1)*train_percentage)\n",
        "#Ltrain_samples2 = int(len(tweets2)*train_percentage)\n",
        "#Ltrain_samples3 = int(len(tweets3)*train_percentage)\n",
        "\n",
        "word_features1 = get_word_features(get_words_in_tweets(tweets1))#tweets1[0:Ltrain_samples1-1]\n",
        "word_features2 = get_word_features(get_words_in_tweets(tweets2))#tweets2[0:Ltrain_samples2-1]\n",
        "word_features3 = get_word_features(get_words_in_tweets(tweets3))#tweets3[0:Ltrain_samples3-1]\n",
        "\n",
        "training_set1 = nltk.classify.apply_features(extract_features1, tweets1)#[0:Ltrain_samples1-1]\n",
        "training_set2 = nltk.classify.apply_features(extract_features2, tweets2)#[0:Ltrain_samples2-1]\n",
        "training_set3 = nltk.classify.apply_features(extract_features3, tweets3)#[0:Ltrain_samples3-1]\n",
        "time1=time()\n",
        "classifier1 = nltk.classify.SklearnClassifier(LinearSVC()).train(training_set1)\n",
        "classifier2 = nltk.classify.SklearnClassifier(LinearSVC()).train(training_set2)\n",
        "classifier3 = nltk.classify.SklearnClassifier(LinearSVC()).train(training_set3)\n",
        "time2=time()\n",
        "print(\"Training time: \", time2-time1)\n",
        "testing_set1 = nltk.classify.apply_features(extract_features1, ttweets1)#tweets1[Ltrain_samples1:]\n",
        "testing_set2 = nltk.classify.apply_features(extract_features2, ttweets2)#tweets2[Ltrain_samples2:]\n",
        "testing_set3 = nltk.classify.apply_features(extract_features3, ttweets3)#tweets3[Ltrain_samples3:]\n",
        "\n",
        "final_test = ttweets1 + ttweets2 + ttweets3#final_test = tweets1[Ltrain_samples1:] + tweets2[Ltrain_samples2:] + tweets3[Ltrain_samples3:]\n",
        "\n",
        "print(\"Classifier 1 training accuracy: \",(nltk.classify.accuracy(classifier1, training_set1))*100)#training accuracy of the positive vs negative classifier\n",
        "acc = [0, 0, 0]#vector of testing accuracies, will be used in voter function\n",
        "acc[0]=nltk.classify.accuracy(classifier1, testing_set1)\n",
        "print(\"Classifier 1 testing accuracy: \",(acc[0]*100))#testing accuracy of the positive vs negative classifier\n",
        "\n",
        "print(\"Classifier 2 training accuracy: \",(nltk.classify.accuracy(classifier2, training_set2))*100)\n",
        "acc[1]=nltk.classify.accuracy(classifier2, testing_set2)\n",
        "print(\"Classifier 2 testing accuracy: \",(acc[1]*100))\n",
        "\n",
        "print(\"Classifier 3 training accuracy: \",(nltk.classify.accuracy(classifier3, training_set3))*100)\n",
        "acc[2]=nltk.classify.accuracy(classifier3, testing_set3)\n",
        "print(\"Classifier 3 testing accuracy: \",(acc[2]*100))\n",
        "\n",
        "def voter(tweet_test, c1, c2, c3, acc):#this voting function will give the final classification\n",
        "  prediction = ['0', '0', '0']\n",
        "  prediction[0] = c1.classify(extract_features1((tweet_test)))\n",
        "  prediction[1] = c2.classify(extract_features2((tweet_test)))\n",
        "  prediction[2] = c3.classify(extract_features3((tweet_test)))\n",
        "  if prediction[0] == prediction[1]:  #those ifs look if two classifiers are giving the same result. In that case (2/3 voting same category), that will be the output.\n",
        "    return prediction[0]\n",
        "  elif prediction[0] == prediction[2]:\n",
        "    return prediction[0]\n",
        "  elif prediction[1] == prediction[2]:\n",
        "    return prediction[1]\n",
        "  else:\n",
        "    return prediction[acc.index(max(acc))]    #if any classifier coincides, then it means that all 3 give a different class. In that case, the chosen one will be that of the classifier with more accuracy.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "full tweets:  10768\n",
            "Positive vs negative tweets (training):  6634\n",
            "Positive vs negative tweets (testing):  789\n",
            "Neutral vs negative tweets (training):  6685\n",
            "Neutral vs negative tweets (testing):  855\n",
            "Positive vs neutral tweets (training):  3215\n",
            "Positive vs neutral tweets (testing):  855\n",
            "Training time:  202.2280502319336\n",
            "Classifier 1 training accuracy:  99.92463069038287\n",
            "Classifier 1 testing accuracy:  80.6083650190114\n",
            "Classifier 2 training accuracy:  99.3866866118175\n",
            "Classifier 2 testing accuracy:  75.55555555555556\n",
            "Classifier 3 training accuracy:  99.50233281493001\n",
            "Classifier 3 testing accuracy:  74.2603550295858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbSqcrFOh9_t",
        "outputId": "e8453709-536f-4e69-9f1b-b305ffe0803e"
      },
      "source": [
        "final_test = ttweets1 + ttweets2 + ttweets3   #with this function we obtain the final accuracy that takes into account the voting.\n",
        "#final_test = tweets1[Ltrain_samples1:] + tweets2[Ltrain_samples2:] + tweets3[Ltrain_samples3:]\n",
        "correct_counter = 0\n",
        "for t in final_test:\n",
        "  v = voter(t[0], classifier1, classifier2, classifier3, acc)\n",
        "  if v == t[-1]:\n",
        "    correct_counter = correct_counter + 1\n",
        "    #print(v, \" \", t[-1],\": correct\")\n",
        "  #else:\n",
        "    #print(v, \" \", t[-1],\": incorrect\")\n",
        "final_acc = (correct_counter/len(final_test))*100\n",
        "print(\"Final accuracy: \", final_acc, \" %\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final accuracy:  65.94827586206897  %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THXOV4lI8jxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b709f60-3be9-4e36-c121-c1fcee4c8df3"
      },
      "source": [
        "tweet = \"Service it's ok\"\n",
        "print(voter(tweet,classifier1, classifier2, classifier3, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neutral\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}